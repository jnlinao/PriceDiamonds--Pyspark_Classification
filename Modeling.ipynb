{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "restricted-syntax",
   "metadata": {},
   "source": [
    "# Project Milestone III: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-powell",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-chase",
   "metadata": {},
   "source": [
    "Here we will install our visualization packages to use for histograms, bar charts, and scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "!emr/notebook-env/bin/pip install bokeh\n",
    "!emr/notebook-env/bin/pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-democrat",
   "metadata": {},
   "source": [
    "We see that the packages were successfully installed. We need to restart the kernal to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-thomas",
   "metadata": {},
   "source": [
    "Here we import pyspark and pyspark sql to start the spark application to use for our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,sum\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.ml.feature as feat\n",
    "import numpy as np\n",
    "import pyspark.ml.stat as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.ml.regression as rg\n",
    "from pyspark.ml.regression import LinearRegression, LinearRegressionSummary\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.ml.evaluation as ev\n",
    "import pyspark.ml.tuning as tune\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-stanley",
   "metadata": {},
   "source": [
    "We see that the Spark application was sucessfully started and we can use Spark now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-jerusalem",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-strap",
   "metadata": {},
   "source": [
    "We load our dataset from our s3 bucket into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df = (\n",
    "    spark.read.csv('s3://samuell-cis4567/Spark/diamonds/diamonds.csv', \n",
    "    header=True, \n",
    "    inferSchema=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-philip",
   "metadata": {},
   "source": [
    "We have successfully loaded our data into Pyspark dataframe. We will use this dataframe for our cleansing before analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-annotation",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-premiere",
   "metadata": {},
   "source": [
    "### Checking and Removing Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-japan",
   "metadata": {},
   "source": [
    "In the following code, we want to check our dataset to see we have duplicate rows. Duplicate rows would be an issue and would result in us taking action to remove it. We do this by comparing the total count of all records and compare it with the distinct records in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df.count(), diamonds_df.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-radius",
   "metadata": {},
   "source": [
    "From the output, we see that there are no duplicate records as the counts are the same. So we do not have to take action to remove any duplicated records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-replacement",
   "metadata": {},
   "source": [
    "### Checking and Removing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-concrete",
   "metadata": {},
   "source": [
    "We run the following code to see if there are any missing values in any of our rows within our columns of our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in diamonds_df.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-asbestos",
   "metadata": {},
   "source": [
    "From the output, we see that there are no missing values. So, we do need to take action to clean our dataset to accomodate any rows that have missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-massage",
   "metadata": {},
   "source": [
    "## Outlier Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-massachusetts",
   "metadata": {},
   "source": [
    "### Carat Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-alcohol",
   "metadata": {},
   "source": [
    "We begin cleansing our data by removing outliers. We do this by observing the visualization we made via the carat histogram. We see that there are outliers that are greater than the value 2.6. We want to filter on our dataframe to values less than 2.6, where most of our data is found for this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df = (\n",
    "    diamonds_df\n",
    "    .filter(diamonds_df.carat < 2.6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-lemon",
   "metadata": {},
   "source": [
    "By reassigning this filtered dataframe to our diamonds_df, this new dataframe will represent the filter we had previously specified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-feeding",
   "metadata": {},
   "source": [
    "### Depth Column "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-undergraduate",
   "metadata": {},
   "source": [
    "We begin cleansing our data by removing outliers. We do this by observing the visualization we made via the depth histogram. We see that there are outliers that are less than the value 50.2. We want to filter on our dataframe to values greater than 50.2, where most of our data is found for this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df = (\n",
    "    diamonds_df\n",
    "    .filter(diamonds_df.depth > 50.2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-moscow",
   "metadata": {},
   "source": [
    "By reassigning this filtered dataframe to our diamonds_df, this new dataframe will represent the filter we had previously specified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-barcelona",
   "metadata": {},
   "source": [
    "We begin cleansing our data by removing outliers. We do this by observing the visualization we made via the depth histogram. We see that there are outliers that are greater than the value 71.8. We want to filter on our dataframe to values less than 71.8, where most of our data is found for this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df = (\n",
    "    diamonds_df\n",
    "    .filter(diamonds_df.depth < 71.8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-synthetic",
   "metadata": {},
   "source": [
    "By reassigning this filtered dataframe to our diamonds_df, this new dataframe will represent the filter we had previously specified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-listing",
   "metadata": {},
   "source": [
    "### Table Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-maria",
   "metadata": {},
   "source": [
    "We begin cleansing our data by removing outliers. We do this by observing the visualization we made via the table histogram. We see that there are outliers that are less than the value 49. We want to filter on our dataframe to values greater than 49, where most of our data is found for this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df = (\n",
    "    diamonds_df\n",
    "    .filter(diamonds_df.table > 49)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-milan",
   "metadata": {},
   "source": [
    "By reassigning this filtered dataframe to our diamonds_df, this new dataframe will represent the filter we had previously specified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-converter",
   "metadata": {},
   "source": [
    "We begin cleansing our data by removing outliers. We do this by observing the visualization we made via the table histogram. We see that there are outliers that are greater than the value 70. We want to filter on our dataframe to values less than 70, where most of our data is found for this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df = (\n",
    "    diamonds_df\n",
    "    .filter(diamonds_df.table < 70)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-fourth",
   "metadata": {},
   "source": [
    "By reassigning this filtered dataframe to our diamonds_df, this new dataframe will represent the filter we had previously specified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-department",
   "metadata": {},
   "source": [
    "### Z Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-tampa",
   "metadata": {},
   "source": [
    "We begin cleansing our data by removing outliers. We do this by observing the visualization we made via the z histogram. We see that there are outliers that are less than or equal to the value 0. We want to filter on our dataframe to values greater than 0, where most of our data is found for this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df = (\n",
    "    diamonds_df\n",
    "    .filter(diamonds_df.z > 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-importance",
   "metadata": {},
   "source": [
    "By reassigning this filtered dataframe to our diamonds_df, this new dataframe will represent the filter we had previously specified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-merchant",
   "metadata": {},
   "source": [
    "We begin cleansing our data by removing outliers. We do this by observing the visualization we made via the z histogram. We see that there are outliers that are greater than the value 6.36. We want to filter on our dataframe to values less than 6.36, where most of our data is found for this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df = (\n",
    "    diamonds_df\n",
    "    .filter(diamonds_df.z < 6.36)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-accommodation",
   "metadata": {},
   "source": [
    "By reassigning this filtered dataframe to our diamonds_df, this new dataframe will represent the filter we had previously specified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-cloud",
   "metadata": {},
   "source": [
    "### Y Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-excellence",
   "metadata": {},
   "source": [
    "We begin cleansing our data by removing outliers. We do this by observing the visualization we made via the y histogram. We see that there are outliers that are less than or equal to the value 0. We want to filter on our dataframe to values greater than 0, where most of our data is found for this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df = (\n",
    "    diamonds_df\n",
    "    .filter(diamonds_df.y > 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-selling",
   "metadata": {},
   "source": [
    "By reassigning this filtered dataframe to our diamonds_df, this new dataframe will represent the filter we had previously specified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-interpretation",
   "metadata": {},
   "source": [
    "We begin cleansing our data by removing outliers. We do this by observing the visualization we made via the y histogram. We see that there are outliers that are greater than the value 10. We want to filter on our dataframe to values less than 10, where most of our data is found for this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df = (\n",
    "    diamonds_df\n",
    "    .filter(diamonds_df.y < 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-qualification",
   "metadata": {},
   "source": [
    "By reassigning this filtered dataframe to our diamonds_df, this new dataframe will represent the filter we had previously specified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-capability",
   "metadata": {},
   "source": [
    "### X Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-tyler",
   "metadata": {},
   "source": [
    "We begin cleansing our data by removing outliers. We do this by observing the visualization we made via the x histogram. We see that there are outliers that are less than or equal to the value 0. We want to filter on our dataframe to values greater than 0, where most of our data is found for this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df = (\n",
    "    diamonds_df\n",
    "    .filter(diamonds_df.x > 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-adobe",
   "metadata": {},
   "source": [
    "By reassigning this filtered dataframe to our diamonds_df, this new dataframe will represent the filter we had previously specified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-roulette",
   "metadata": {},
   "source": [
    "We begin cleansing our data by removing outliers. We do this by observing the visualization we made via the x histogram. We see that there are outliers that are greater than the value 8.5. We want to filter on our dataframe to values less than 8.5, where most of our data is found for this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df = (\n",
    "    diamonds_df\n",
    "    .filter(diamonds_df.x < 8.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-scanning",
   "metadata": {},
   "source": [
    "By reassigning this filtered dataframe to our diamonds_df, this new dataframe will represent the filter we had previously specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b541cb9",
   "metadata": {},
   "source": [
    "We begin cleansing our data by removing outliers. We do this by observing the visualization we made via the price histogram. We see that there are outliers that are greater than the value around 12,000. We want to filter on our dataframe to values less than 12,000, where most of our data is found for this column. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5431ae",
   "metadata": {},
   "source": [
    "## Price Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901cd79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df = (\n",
    "    diamonds_df\n",
    "    .filter(diamonds_df.price < 12000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22716cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76f3724",
   "metadata": {},
   "source": [
    "After finalizing outlier analysis, we want to see that only 3% of our data has been removed. We take the count of our data to see how much data remains. We can see that less than 3% of the data has been filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4946f81",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de3174",
   "metadata": {},
   "source": [
    "For normalization, we attempted MinMax Scaler on our numerical data, both with the label (price) and without it. Through this we recevied poor model results including an R^2 score of 0. When we pursued a model with non normalized data as the following code convey, we were able to develop a model that proved reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-devon",
   "metadata": {},
   "source": [
    "## Dummy Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-roberts",
   "metadata": {},
   "source": [
    "In our dataset, we have some categorical variables that need to converted to dummy variables in order to transform and fit it in our model. In the following code, we will print the schema of our dataframe in order to double check which of our columns are string times, hence need to converted into a dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-feeding",
   "metadata": {},
   "source": [
    "From this output, we see the columns that we need to convert into dummy variables: cut, color, and clarity. Since they are categorical variables, we need to design code that will fulfill this conversion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-superior",
   "metadata": {},
   "source": [
    "In the following code we assign the names of those columns that we need to convert by putting it in a list. We will implement a for loop that will look at the distinct values in each of the columns and add each new column for every distinct value for each of the defined categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns=[\"cut\", \"color\", \"clarity\"]\n",
    "expressions=[]\n",
    "for column in categorical_columns:\n",
    "    expression = diamonds_df.select(column).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    types_expr = [f.when(f.col(column) == ty, 1).otherwise(0).alias(\"e_{}_{}\".format(column, ty)) for ty in expression]\n",
    "    expressions += types_expr\n",
    "diamonds_dummy = diamonds_df.select(\"_c0\", *categorical_columns, *expressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-sarah",
   "metadata": {},
   "source": [
    "After running the previous cell, we have our new dataframe containing our dummy variables. In the following code, we want to see our new dataframe and take note of the current columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_dummy.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-beaver",
   "metadata": {},
   "source": [
    "Looking our the first three rows of our dataframe, we see that the code successfully created dummy variables for our categorical variables. However, we need to add in our numerical columns and also remove the original variables (cut, color, clarity) while keeping its new dummy variable counterparts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-favor",
   "metadata": {},
   "source": [
    "In the following code, we want to start creating our dataframe with just the dummy variables and the numerical columns. To begin, we will drop the original categorical variables from the dummy variable dataframe. This ensures that when we create the new dataframe, we do not run into a duplicate column error (since we are going to merge this dummy dataframe with the original dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_dummy = diamonds_dummy.drop(\"cut\", \"color\", \"clarity\")\n",
    "diamonds_dummy.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-newfoundland",
   "metadata": {},
   "source": [
    "Looking at the output we see that the categorical variables were removed and the dummy variables along with the unique ID column are left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-board",
   "metadata": {},
   "source": [
    "In the following code, we perform a join -- our original dataset with the categorical variables and all the numerical columns and the dummy variable dataset. We do this join on the unique ID column, which we will then drop for our subsequent modeling steps. We will reassign this new dataframe to diamond_df, where we will then drop the categorical variables in order to only include the dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df = diamonds_dummy.join(diamonds_df, on=[\"_c0\"]).drop(\"_c0\")\n",
    "diamonds_df = diamonds_df.drop(\"cut\", \"color\", \"clarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-walnut",
   "metadata": {},
   "source": [
    "In the following code, we take another look at the columns in the final dataset we will be using. Here we use n dummies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-jacksonville",
   "metadata": {},
   "source": [
    "From the output, we have all our n dummy variables for each of the previous categorical columns and additionally the numeric variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-ozone",
   "metadata": {},
   "source": [
    "## Correlation Matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-wound",
   "metadata": {},
   "source": [
    "In order to We need to install S3 package needed for saving correlation output to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "!emr/notebook-env/bin/pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package('s3fs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-insert",
   "metadata": {},
   "source": [
    "We see that the packages were successfully installed. We need to restart the kernal to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-mistress",
   "metadata": {},
   "source": [
    "In order to begin our process of building a Machine Learning model, we need to combine and merge all our dependent variables into a single column. To do this, we need to use VectorAssembler. VectorAssembler is a feature transformer that merges multiple columns into a vector column. With this code, we use VectorAssembler on all of our columns. Following this, we create the correlation matric using our VectorAssembler. Finally, we export our correlation matrix to our S3 bucket, which we will view as a .csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%spark -o corr\n",
    "\n",
    "features_and_label = (\n",
    "    feat.VectorAssembler(\n",
    "        inputCols=diamonds_df.columns, \n",
    "        outputCol='features'\n",
    "    )\n",
    ")\n",
    "\n",
    "corr = st.Correlation.corr(\n",
    "    features_and_label.transform(diamonds_df), \n",
    "    'features', \n",
    "    'pearson'\n",
    ")\n",
    "\n",
    "print(str(corr.collect()[0][0]))\n",
    "corr_pd = corr.toPandas()\n",
    "output_np = np.array(corr_pd.iloc[0, 0].values).reshape(\n",
    "    (corr_pd.iloc[0, 0].numRows, corr_pd.iloc[0, 0].numCols))\n",
    "\n",
    "#Change the following path to a path in your own S3 bucket\n",
    "spark.createDataFrame(pd.DataFrame(output_np)).repartition(1).write.format('csv').option('header',True\n",
    "                ).mode('overwrite').option('sep',',').save('s3://jnlinao-bucket-1/Spark/Exports/corr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd4a40",
   "metadata": {},
   "source": [
    "Here we define our final_df which we will use in our modeling. We select all columns that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eab8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = diamonds_df.select(\"price\", \"carat\",\"depth\", \"table\", \"x\", \"y\", \"z\",\n",
    "                             'e_cut_Fair', 'e_cut_Premium', 'e_cut_Good', 'e_cut_Ideal', \n",
    "                              'e_cut_Very Good', 'e_color_D', 'e_color_G', 'e_color_J', \n",
    "                              'e_color_H', 'e_color_F', 'e_color_I', 'e_color_E', 'e_clarity_VVS2', \n",
    "                              'e_clarity_VS2', 'e_clarity_SI1', 'e_clarity_I1', 'e_clarity_SI2', \n",
    "                              'e_clarity_IF', 'e_clarity_VVS1', 'e_clarity_VS1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e5258",
   "metadata": {},
   "source": [
    "In the below cell, we use vector assembler to define our input and output columns. Our input column is price and our output columns are all the features or other columns. Next, we cast the price column to a float type using the previous vector assembler and select all the columns. Next, we create a linear regression object using pyspark.ml.regression as rg and fit it to our previous price_dataset. Finally, we view the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b7b27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols = final_df.columns[1:]\n",
    "    , outputCol= 'features')\n",
    "\n",
    "price_dataset = (\n",
    "    vectorAssembler\n",
    "    .transform(final_df)\n",
    "    .withColumn(\n",
    "        'label'\n",
    "        , f.col('Price').cast('float'))\n",
    "    .select('label', 'features')\n",
    ")\n",
    "\n",
    "lr_obj = rg.LinearRegression(\n",
    "    maxIter=10\n",
    "    , regParam=0.01\n",
    "    , elasticNetParam=1.00)\n",
    "lr_model = lr_obj.fit(price_dataset)\n",
    "\n",
    "\n",
    "lr_model.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94ecdf",
   "metadata": {},
   "source": [
    "We examine the model's summary by using from pyspark.ml.regression import LinearRegression, LinearRegressionSummary and assigning it to an object called summary. We then print the r^2, RMSE, MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.coefficients\n",
    "summary = lr_model.summary\n",
    "\n",
    "print(\n",
    "    summary.r2\n",
    "    , summary.rootMeanSquaredError\n",
    "    , summary.meanAbsoluteError\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d6fde9",
   "metadata": {},
   "source": [
    "First, we create the same vector assembler from the previous cell. Next, we create a linear regression object by using GeneralizedLinearRegression. We set the labelCol to our dependent variable price. We define link to identity because we do not transform price. Next, we create a pipeline of the previous two objects: vectorAssembler and lr_obj. Finally, we run the pipeline on our final_df and show the actual prices and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be405382",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols = final_df.columns[1:]\n",
    "    , outputCol='features')\n",
    "\n",
    "lr_obj = rg.GeneralizedLinearRegression(\n",
    "    labelCol='price'\n",
    "    , maxIter=10\n",
    "    , regParam=0.01\n",
    "    , link='identity'\n",
    "    , linkPredictionCol=\"p\"\n",
    ")\n",
    "\n",
    "pip = Pipeline(stages=[vectorAssembler, lr_obj])\n",
    "\n",
    "(\n",
    "    pip\n",
    "    .fit(final_df)\n",
    "    .transform(final_df)\n",
    "    .select('price', 'prediction')\n",
    "    .show(50)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a75d84",
   "metadata": {},
   "source": [
    "In this cell, we define a model object that fits the pipeline to our final_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c287a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pip.fit(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b867e",
   "metadata": {},
   "source": [
    "In this cell, we transform the final_df and assign it to object prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208bd136",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4ad37",
   "metadata": {},
   "source": [
    "In this cell, we select the price and prediction columns and show 50 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a1b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.select(\"price\", \"prediction\").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6922f045",
   "metadata": {},
   "source": [
    "In this cell, we utilize from pyspark.ml.evaluation import RegressionEvaluator to create a RegressionEvaluator object called eval. We define our labelCol as the price column and our predictionCol as the prediction column from the previous prediction object. We then display RMSE, MSE, MAE, and r^2 by evaluating the predictions respective towards a certain metric. We also format the metrics to 3 decimal spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc6cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Root Mean Square Error\n",
    "rmse = eval.evaluate(prediction)\n",
    "print(\"RMSE: %.3f\" % rmse)\n",
    "\n",
    "# Mean Square Error\n",
    "mse = eval.evaluate(prediction, {eval.metricName: \"mse\"})\n",
    "print(\"MSE: %.3f\" % mse)\n",
    "\n",
    "# Mean Absolute Error\n",
    "mae = eval.evaluate(prediction, {eval.metricName: \"mae\"})\n",
    "print(\"MAE: %.3f\" % mae)\n",
    "\n",
    "#r2 - coefficient of determination\n",
    "r2 = eval.evaluate(prediction, {eval.metricName: \"r2\"})\n",
    "print(\"r2: %.3f\" %r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38398b4",
   "metadata": {},
   "source": [
    "In this cell, we create a gradient boosted tree object using import pyspark.ml.regression as rg. Our labelCol is our dependent variable price. Next, we create a pipeline object using the previous vector assembler and the new gbt_obj we created in this cell. We then fit and transform the pipeline to our final_df, select the price and prediction columns, and assign it to an object called results. Finally, we create an evaluator object by using import pyspark.ml.evaluation as ev and assign our dependent variable price to the labelCol. We then show r^2 by evaluating our results object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28604219",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_obj = rg.GBTRegressor(\n",
    "    labelCol='price'\n",
    "    , minInstancesPerNode=10\n",
    "    , minInfoGain=0.1\n",
    ")\n",
    "\n",
    "pip = Pipeline(stages=[vectorAssembler, gbt_obj])\n",
    "\n",
    "results = (\n",
    "    pip\n",
    "    .fit(final_df)\n",
    "    .transform(final_df)\n",
    "    .select('price', 'prediction')\n",
    ")\n",
    "\n",
    "evaluator = ev.RegressionEvaluator(labelCol='price')\n",
    "evaluator.evaluate(results, {evaluator.metricName: 'r2'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a10cf6",
   "metadata": {},
   "source": [
    "In this cell, we create a gradient boosted tree object using import pyspark.ml.regression as rg. Our labelCol is our dependent variable price. Next, we create a pipeline object using the previous vector assembler and the new gbt_obj we created in this cell. We then fit and transform the pipeline to our final_df, select the price and prediction columns, and assign it to an object called results. Finally, we create an evaluator object by using import pyspark.ml.evaluation as ev and assign our dependent variable price to the labelCol. We then show RMSE by evaluating our results object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771a1efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_obj = rg.GBTRegressor(\n",
    "    labelCol='price'\n",
    "    , minInstancesPerNode=10\n",
    "    , minInfoGain=0.1\n",
    ")\n",
    "\n",
    "pip = Pipeline(stages=[vectorAssembler, gbt_obj])\n",
    "\n",
    "results = (\n",
    "    pip\n",
    "    .fit(final_df)\n",
    "    .transform(final_df)\n",
    "    .select('price', 'prediction')\n",
    ")\n",
    "\n",
    "evaluator = ev.RegressionEvaluator(labelCol='price')\n",
    "evaluator.evaluate(results, {evaluator.metricName: 'rmse'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9f5575",
   "metadata": {},
   "source": [
    "In this cell, we create a gradient boosted tree object using import pyspark.ml.regression as rg. Our labelCol is our dependent variable price. Next, we create a pipeline object using the previous vector assembler and the new gbt_obj we created in this cell. We then fit and transform the pipeline to our final_df, select the price and prediction columns, and assign it to an object called results. Finally, we create an evaluator object by using import pyspark.ml.evaluation as ev and assign our dependent variable price to the labelCol. We then show MSE by evaluating our results object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_obj = rg.GBTRegressor(\n",
    "    labelCol='price'\n",
    "    , minInstancesPerNode=10\n",
    "    , minInfoGain=0.1\n",
    ")\n",
    "\n",
    "pip = Pipeline(stages=[vectorAssembler, gbt_obj])\n",
    "\n",
    "results = (\n",
    "    pip\n",
    "    .fit(final_df)\n",
    "    .transform(final_df)\n",
    "    .select('price', 'prediction')\n",
    ")\n",
    "\n",
    "evaluator = ev.RegressionEvaluator(labelCol='price')\n",
    "evaluator.evaluate(results, {evaluator.metricName: 'mse'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80681650",
   "metadata": {},
   "source": [
    "In this cell, we create a gradient boosted tree object using import pyspark.ml.regression as rg. Our labelCol is our dependent variable price. Next, we create a pipeline object using the previous vector assembler and the new gbt_obj we created in this cell. We then fit and transform the pipeline to our final_df, select the price and prediction columns, and assign it to an object called results. Finally, we create an evaluator object by using import pyspark.ml.evaluation as ev and assign our dependent variable price to the labelCol. We then show MAE by evaluating our results object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_obj = rg.GBTRegressor(\n",
    "    labelCol='price'\n",
    "    , minInstancesPerNode=10\n",
    "    , minInfoGain=0.1\n",
    ")\n",
    "\n",
    "pip = Pipeline(stages=[vectorAssembler, gbt_obj])\n",
    "\n",
    "results = (\n",
    "    pip\n",
    "    .fit(final_df)\n",
    "    .transform(final_df)\n",
    "    .select('price', 'prediction')\n",
    ")\n",
    "\n",
    "evaluator = ev.RegressionEvaluator(labelCol='price')\n",
    "evaluator.evaluate(results, {evaluator.metricName: 'mae'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a24a505",
   "metadata": {},
   "source": [
    "## Hypertuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c77e8",
   "metadata": {},
   "source": [
    "In the following cell, we split our data into train and test with 70% of the dataset being in the training dataset and the rest of the 30% in the test set. Doing this will fit our models to the training data which it will later be validated on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00c176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(diamonds_train, diamonds_test) = final_df.randomSplit([.7, .3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc5b0b",
   "metadata": {},
   "source": [
    "After splitting our data, we will use the training data to train the classifiers with the different parameters in the pipeline. Then later on the test set results will provide the model evaluation metrics from which we can identify the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ddfc86",
   "metadata": {},
   "source": [
    "In the next cell, we develop our pipelines for VectorAssembler, ChiSqSelector, and the Linear Regression Model. We are defining the parameters for each of these objects so that we can later fit this into our pipeline. However for the Linear Regression Model, we are going to build a Grid Search for the parameters RegParam and ElasticNetParam. We will run this Grid Search with different values for these parameters. At the end of the cell, we will fit the pipeline containing VectorAssembler and the Linear Regression Object to the training data. This data transformation will then be transformed with the Grid Search Cross Validator and the Regression Validator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols = final_df.columns[1:]\n",
    "    , outputCol='features')\n",
    "\n",
    "\n",
    "selector = feat.ChiSqSelector(\n",
    "    labelCol='price'\n",
    "    , numTopFeatures=5\n",
    "    , outputCol='selected')\n",
    "\n",
    "lr_obj = rg.LinearRegression(\n",
    "    labelCol='price'\n",
    "    , featuresCol=selector.getOutputCol()\n",
    ")\n",
    "\n",
    "\n",
    "linReg_grid = (\n",
    "    tune.ParamGridBuilder()\n",
    "    .addGrid(lr_obj.regParam\n",
    "            , [0.01, 0.1]\n",
    "        )\n",
    "    .addGrid(lr_obj.elasticNetParam\n",
    "            , [1.0, 0.5]\n",
    "        )\n",
    "    .build()\n",
    ")\n",
    "\n",
    "linReg_ev = ev.RegressionEvaluator(labelCol='price', predictionCol='prediction')\n",
    "\n",
    "cross_v = tune.CrossValidator(\n",
    "    estimator=lr_obj\n",
    "    , estimatorParamMaps=linReg_grid\n",
    "    , evaluator=linReg_ev\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorAssembler, selector])\n",
    "data_trans = pipeline.fit(diamonds_train)\n",
    "\n",
    "linReg_modelTest = cross_v.fit(\n",
    "    data_trans.transform(diamonds_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117b4ce",
   "metadata": {},
   "source": [
    "We acknowledge the model results for performance using the varied parameters for Elastic Net Param and Get Param. We use this to determine and identify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b702ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure performance of best model\n",
    "data_trans_test = data_trans.transform(diamonds_test)\n",
    "results = linReg_modelTest.transform(data_trans_test)\n",
    "\n",
    "print(linReg_ev.evaluate(results, {linReg_ev.metricName: 'weightedPrecision'}))\n",
    "print(linReg_ev.evaluate(results, {linReg_ev.metricName: 'weightedRecall'}))\n",
    "print(linReg_ev.evaluate(results, {linReg_ev.metricName: 'accuracy'}))\n",
    "print('Best params, regParam: %s, elasticNetParam: %s' \n",
    "      %(linReg_modelTest.bestModel._java_obj.getRegParam(),\n",
    "      linReg_modelTest.bestModel._java_obj.getElasticNetParam()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7100dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70856b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
